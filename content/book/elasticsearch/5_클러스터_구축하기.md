# 2주차 (p109 ~ p144)

## elasticsearch.yml
- cluster
```yml
cluster.name: es-search
```
- node
```yml
# data-group1
node.name: unifiedsearch-group1-ec2id
node.roles: [ data ]
node.attr.data: group1
```
```yml
# data-group2
node.name: unifiedsearch-group2-ec2id
node.roles: [ data ]
node.attr.data: group2
```
```yml
# coordinating
node.name: search-coordinating-indexer
node.roles: []
```

> `node.roles` -> 자동으로 힙메모리 할당 
>  
> coordinate node: aggs등 연산 담당
> data랑 coordinate 분리하면 통신을 한 번 더 해야하니 느려질 수 있음
> 그러나 역할 분리
> cerebro > uptime
> 임계값에 따라 늘리고 줄이고 하는 중
> 

- memory
```yml
bootstrap.memory_lock: true 
```
시스템의 스왑 메모리 영역을 사용하지 않도록 하는 설정
메모리끼리 교환 방지

- network
```yml
network.host: 0.0.0.0
http.port: 11200 # 외부와 통신
transport.tcp.port: 11300 # 노드간 통신
http.max_content_length: 1024mb
```

- discovery
```yml
discovery.seed_hosts: ["ip list"] # master ip port
cluster.initial_master_nodes: ["search-master1", "search-master2", "search-master3"] # master node name
```

> **split brain 현상 방지**
> - minimum_master_nodes값을 과반수로 설정
> - (total number of master-eligible nodes / 2) + 1
 

->  7.0 부터는 discovery.zen.minimum_master_nodes 설정이 사라지고 대신 node.master: true 인 노드가 추가되면 클러스터가 스스로 minimum_master_nodes 노드 값을 변경하도록 되었습니다. 
- 사용자는 최초 마스터 후보로 선출할 cluster.initial_master_nodes: [ ] 값만 설정하면 됩니다.


노드 역할 분리
- 요청을 받는 노드와 색인 및 검색에 사용하는 노드를 분리
- 힙메모리 리소스 확보

- buffer
hybridfs: 파일 시스템


-xpack
-> monitoring 및 보안 
-> 거의 유료




## jvm.options 설정파일
- xms: 최소값
- xmx: 최대값
  - 같게 설정 권고

- 32기가 언더로
- 전체 메모리의 절반을 힙메모리로 할당

```text
_cat/health
epoch      timestamp cluster   status node.total node.data shards pri relo init unassign pending_tasks max_task_wait_time active_shards_percent
1664885334 12:08:54  es-search green          23        11    973 479    0    0        0             0                  -                100.0%
```

unassigned shard
- 샤드가 어떤 노드에도 배치되지 못한 상태


g1gc
-> 14 버전이후는 g1gc (우리는 16 버전)